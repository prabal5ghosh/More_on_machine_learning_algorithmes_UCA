{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5c9f64",
   "metadata": {},
   "source": [
    "# PRABAL GHOSH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2aa848",
   "metadata": {},
   "source": [
    "# LAB - NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4d087-0984-478c-8761-103f42756a01",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The naive Bayes classifier\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A supervised classification method, with a lot of assumptions ..., but well adapted to high dimensional data with moderate sample size. \n",
    "\n",
    "As reminder:\n",
    "- logistic regression puts a model on $P(y|x)$ and estimates the parameters by minimizing the cross-entropy loss\n",
    "- SVM puts a model only on the classification boundary and research the separating hyperplane with largest margin \n",
    "\n",
    "Another philosophy is to consider a generative model on data distribution, i.e. model $P(x,y)$ by writting $P(x,y) = P(y)P(x|y)$. Thus it is only needed to estimate $P(x|y)$ for each possible value of $y$ (multivariate gaussian distribution, multivariate multinomial distribution, ...), and also $P(y)$ the prior probabilities (typically simply the proportion of each class). Then the posterior probability $P(y|x)$ can be computed by using the **Bayes theorem**: \n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(y)P(x|y)}{P(x)} = \\frac{P(y)P(x|y)}{\\sum_{k=1}^{K} P(y=k) P(x|y=k)}\n",
    "$$\n",
    "\n",
    "\n",
    "## The model\n",
    "\n",
    "Let assume that $x \\in \\mathcal{X}$ : \n",
    "- **Continuous features**: $\\mathcal{X}=\\mathcal{R}^d$, $P(x|y=k)$ is modeled by a multivariate density.\n",
    "- **Categorical features**: $\\mathcal{X}=\\prod_{j=1}^{d}\\{1, ..., m_j\\}$, $P(x|y=k)$ is modeled by a multivariate probability.\n",
    "- Also possible **mixed data**: both continuous and categorical\n",
    "\n",
    "**Question**: What model use for $P(x|y=k)$?\n",
    "\n",
    "**Some ideas**:\n",
    "- Multivariate normal distribution (need only to estimate the vector of expectations and the variance covariance matrix) \n",
    "- Mixture of multivariate normal distribution for more flexibility\n",
    "- Not so many models for categorical data ... \n",
    "\n",
    "**The Naives assumtion**:\n",
    "$$\n",
    "P(x|y=k) = P(x_1, \\ldots, x_d |y=k) = \\prod_{j=1}^{d} P(x_j|y=k),\n",
    "$$ \n",
    "\n",
    "It assumes that all the variables are independant given the class. \n",
    "\n",
    "- Avoid to model dependency! \n",
    "- Only need to model $P(x_j|y=k)$ by a univariate distribution\n",
    "    - univariate discrete probability distribution for categorical variables (multinomial, Poisson, ...) \n",
    "    - univariate probability density function (normal, Student, Gamma, exponential, ...)\n",
    "\n",
    "\n",
    "Let assume that $P(x_j|y=k)$ belong to some parametric family we will denote  $P(x_j|y=k) := P(x_j|\\theta_{kj})$  where $\\theta_{kj}$ are the paramters of the distribution of variable $j$ in class $k$:\n",
    "- $\\theta_{kj} = (\\mu_{kj} , \\sigma_{kj}^2)$ for a Gaussian distribution\n",
    "- $\\theta_{kj} = (\\alpha_{kj1}, ..., \\alpha_{kjm_{j}})$ for a multinomial distribution where $\\alpha_{kjh}$ is the probability of the model $h$ of variable $j$ in class $k$\n",
    "\n",
    "\n",
    "## Parameters estimation\n",
    "\n",
    "Parameters are estimated by maximum likelihood as in statistical inference!\n",
    "\n",
    "Let $\\theta_k = (\\theta_{k1}, ..., \\theta_{kd})$ that groups all the parameter of class $k$, let also denote by $\\pi_k = P(y=k)$ the prior probability of class which also need to be estimated. \n",
    "\n",
    "Let denote by $\\theta = (\\pi_1, ..., \\pi_K, \\theta_1, \\ldots, \\theta_K)$ that groups all the paramters of the model \n",
    "\n",
    "The likelihood is: \n",
    "$$\n",
    "\\ell(\\theta) = \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{C}_k} \\log (\\pi_k P(x_i | \\theta_k))\n",
    "$$\n",
    "with $\\mathcal{C}_k$ the set of data points belonging to class $k$\n",
    "\n",
    "\n",
    "**Estimation of the prior probabilities**\n",
    "\n",
    "For all $k \\in \\{1, ..., K \\}$\n",
    "$$\n",
    "\\hat \\pi_k = \\frac{n_k}{n}\n",
    "$$\n",
    "with $n_k$ the number of data in class $k$, among the total of $n$ data. \n",
    "\n",
    "*Remark*: This can depend on the sampling scheme, for instance in the medical setting we can consider restrospective sampling, i.e. fix by advance the proportion of each class in the training data, for instance for rare diseases have the same number of patient with and without the disease in the cohort. In this case the user can give manually the proportion of each class in the whole population.  \n",
    "\n",
    "\n",
    "**Gaussian distribution**\n",
    "For $k \\in \\{1, ..., K\\}$, $j \\in \\{1,...,d\\}$\n",
    "$$\n",
    "\\hat\\mu_{kj} = \\frac{\\sum_{i \\in \\mathcal{C}_k} x_{ij}}{n_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat\\sigma_{kj}^2 = \\frac{\\sum_{i \\in \\mathcal{C}_k} (x_{ij}-\\hat\\mu_{kj})^2}{n_k}\n",
    "$$\n",
    "\n",
    "\n",
    "**Multinomial distribution**\n",
    "For $k \\in \\{1, ..., K\\}$, $j \\in \\{1,...,d\\}$ and $h \\in \\{1,...,m_j\\}$\n",
    "$$\n",
    "\\hat\\alpha_{kjh} = \\frac{n_{kjh}}{n_k}\n",
    "$$\n",
    "with $n_{kjh}$ the number of times that modality $h$ of variable $j$ has been observed in class $k$.\n",
    "\n",
    "*Remark*: Regularized version of the estimators can be consider to avoid to estimate some $\\hat\\alpha_{kjh}$ to $0$. For instance $\\hat\\alpha_{kjh} = \\frac{n_{kjh} + c}{n_k + c m_j}$ where $c>0$ is some regularization parameter.\n",
    "\n",
    "\n",
    "## Prediction of the class for new data\n",
    "\n",
    "Based on $\\hat\\theta$, it is possible to compute $\\forall k \\in \\{1, ..., K\\}$:\n",
    "$$\n",
    "P(y=k|x, \\hat\\theta) = \\frac{\\hat\\pi_k P(x|\\hat\\theta_k)}{\\sum_{k=1}^{K} \\hat\\pi_k P(x|\\hat\\theta_k)}\n",
    "$$\n",
    "\n",
    "Then the predicted class can be obtained by Maximum A Posteriori (MAP)\n",
    "\n",
    "$$\n",
    "\\hat y = \\arg\\max_{k \\in \\{1, ..., K \\}} P(y=k|x, \\hat\\theta)\n",
    "$$\n",
    "\n",
    " \n",
    "## Discussion \n",
    "\n",
    "The model make stong assumptions, i.e. models the distribution of $(x,y)$, where only the distribution of $y|x$ or even only the decision boundary is needed to make prediction. \n",
    "\n",
    "Thus this leds to model miss-specification which may degrade the performance of the final classifier. However such kind of model can still work well with moderate sample size and high number of variables. See for instance [1]\n",
    "\n",
    "[1] Hand, D. J., & Yu, K. (2001). Idiot's Bayesâ€”not so stupid after all?. International statistical review, 69(3), 385-398.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f1c299-61d8-4ed6-a10e-207b21826258",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8157aa0a-fe23-4342-9289-30959038b09f",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes in sklearn\n",
    "\n",
    "More information can be found on :\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "The main functions are :\n",
    "- `GaussianNB` : Naive Bayes for continuous features using Gaussian assumption\n",
    "- `BernoulliNB` : Naive Bayes classifier for multivariate Bernoulli models.\n",
    "- `CategoricalNB` : Naive Bayes classifier for categorical features.\n",
    "- `MultinomialNB` : Naive Bayes classifier for multinomial models.\n",
    "- `ComplementNB` : Complement Naive Bayes classifier.\n",
    "\n",
    "For mixed-type of features there is not dedicated function in sklearn, but continous features can for instances be discretized in order to use `ComplementNB` on the discretized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6246c-756e-4649-aec9-80f1db8300ad",
   "metadata": {},
   "source": [
    "**Q1** : By looking at the documentation explain the differences between `BernoulliNB`, `CategoricalNB` and `MultinomialNB`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21541f44-4f08-4abe-b160-14264aea2676",
   "metadata": {},
   "source": [
    "- **BernoulliNB** =  BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the binarize parameter).\n",
    "\n",
    "- **CategoricalNB** = The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution.\n",
    "\n",
    "\n",
    "\n",
    "- **MultinomialNB**  =  The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab1550-e29a-4edd-a5e0-e77d4e7ef624",
   "metadata": {},
   "source": [
    "## Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46396d53-2c74-4fcb-b51f-a4498c7d4c69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#necessary imports\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f5f96d1-a408-4df4-8b21-6327867fa948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cffdba-d34b-4f70-83e6-c4e6c5edb0bd",
   "metadata": {},
   "source": [
    "**Q2**: Apply the `GaussianNB` to train the model on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc2c87f-1a21-4509-a97b-fa97087ea372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce6918-9454-4a4f-bf68-00476c18b61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bebd3d3-1c06-4ff0-889f-6a7d97aa30e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646cb33",
   "metadata": {},
   "source": [
    "- **class_prior_** attribute represents the prior probability of each class. These are the probabilities assumed for the classes in the absence of any input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdfc1c90-3579-4817-8aec-41ad34715e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean [0.38666667 0.26666667 0.34666667]\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\",classifier.class_prior_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b341e",
   "metadata": {},
   "source": [
    "**here  classes so we got  different prior probability  for classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffde3ac2-7f27-4c2e-9b5e-935003488c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean [[-0.9613413   0.63460144 -1.16137032 -1.13224244]\n",
      " [ 0.08628871 -0.91698259  0.2730818   0.16009988]\n",
      " [ 1.00588937 -0.00245347  1.08531167  1.13973204]]\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\",classifier.theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957fe13",
   "metadata": {},
   "source": [
    "- **theta_** attribute represents the empirical log probability of features given a class, estimated from the training data. Each row corresponds to a class, and each column corresponds to a feature.\n",
    "- This will print the mean of theta_ values for each feature and each class in your trained Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f9e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b730788-cc0b-435a-9462-0a1957ede811",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12313385, 0.75580348, 0.00447543, 0.01245427],\n",
       "       [0.30574543, 0.47437691, 0.07013235, 0.06766935],\n",
       "       [0.46373116, 0.58069278, 0.08600022, 0.07007181]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8baba",
   "metadata": {},
   "source": [
    "- **var_** attribute represents the variance of each feature per class. This attribute is typically used for the calculation of the smoothing parameter in the Naive Bayes model.\n",
    "-  This will print the variances of each feature for each class in your trained Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56691b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ee118cb-42d5-4fcf-88fe-824847c17d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c365c792-98ad-42df-b2b5-0f33fbc32fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd4bf7-c4e5-4e01-9419-45daca63b510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b32379f-f01d-45ab-b278-f6f926041b01",
   "metadata": {},
   "source": [
    "**Q3**: Let consider the following functions\n",
    "- `predict_joint_log_proba(X)`\n",
    "- `predict_log_proba(X)`\n",
    "- `predict_proba(X)`\n",
    "\n",
    "(a) Explain what the above functions do and illustrate it on the iris dataset\n",
    "\n",
    "(b) Explain the use of making use of the log instead of directly computing the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e92fe6a6-0b3d-4fbd-b3e9-e626b2667dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  log is used for stabiliy of the equation \n",
    "#  log of pdf of gaussian is simple to calculate  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768e5fa-523a-4d6d-9ee9-bcf65216c53c",
   "metadata": {},
   "source": [
    "- **predict_joint_log_proba(X)** \n",
    "\n",
    "n*c array \n",
    "\n",
    "- **predict_log_proba(X)**\n",
    "\n",
    "\n",
    "- **predict_proba(X)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552d1cf-abe8-4359-9d2e-4fe0b4c3be85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c60d63c4-ef4a-4e39-a1f3-f5e841642aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "393c59fd-0c0b-464b-97eb-47f308194cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict_joint_log_proba(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f1a29a3-5be0-4d09-9df5-39892c05d1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.43167720e+02, -3.97918268e+00, -2.39083534e+01],\n",
       "       [-2.69379888e+00, -4.34603727e+01, -7.78785652e+01],\n",
       "       [-8.92749339e+02, -1.66887959e+01, -3.56188002e+00],\n",
       "       [ 1.29705871e+00, -3.32067369e+01, -7.28949619e+01],\n",
       "       [-9.46084853e+02, -2.58208089e+01, -3.75647926e+00],\n",
       "       [ 4.51887240e-01, -3.39987306e+01, -7.21168744e+01],\n",
       "       [-4.32323735e+02, -4.37031838e+00, -7.11740700e+00],\n",
       "       [-4.40881603e+02, -3.71506284e+00, -8.59738466e+00],\n",
       "       [-2.72749724e+02, -1.52589200e+00, -1.41876209e+01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict_joint_log_proba(X_test)[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61b90d4b-6c27-4567-bfad-d1458325b681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.39188537e+02, -2.21243779e-09, -1.99291707e+01],\n",
       "       [ 0.00000000e+00, -4.07665738e+01, -7.51847663e+01],\n",
       "       [-8.89187461e+02, -1.31269179e+01, -1.99091363e-06],\n",
       "       [-1.11022302e-15, -3.45037956e+01, -7.41920206e+01],\n",
       "       [-9.42328374e+02, -2.20643297e+01, -2.61567212e-10],\n",
       "       [-1.11022302e-15, -3.44506178e+01, -7.25687616e+01],\n",
       "       [-4.28015560e+02, -6.21427637e-02, -2.80923138e+00],\n",
       "       [-4.37174091e+02, -7.55081626e-03, -4.88987264e+00],\n",
       "       [-2.71223835e+02, -3.17015366e-06, -1.26617321e+01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict_log_proba(X_test)[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "480043d0-cbee-4b3c-a065-ccb8556a484c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.32354372e-104, 9.99999998e-001, 2.21243804e-009],\n",
       "       [1.00000000e+000, 1.97379457e-018, 2.22674692e-033],\n",
       "       [0.00000000e+000, 1.99091165e-006, 9.99998009e-001],\n",
       "       [1.00000000e+000, 1.03559983e-015, 6.00917501e-033],\n",
       "       [0.00000000e+000, 2.61567260e-010, 1.00000000e+000],\n",
       "       [1.00000000e+000, 1.09216133e-015, 3.04640240e-032],\n",
       "       [1.30377984e-186, 9.39748715e-001, 6.02512848e-002],\n",
       "       [1.37310821e-190, 9.92477620e-001, 7.52238046e-003],\n",
       "       [1.61802403e-118, 9.99996830e-001, 3.17014863e-006]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict_proba(X_test)[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cd797",
   "metadata": {},
   "source": [
    "\n",
    "In scikit-learn, these methods are often associated with classifiers for making predictions. Here's what each of these methods does:\n",
    "\n",
    "- **predict_joint_log_proba(X):** This method computes the logarithm of the joint probability density function for the samples in X. The joint probability density function is the probability that a sample belongs to each class, taking into account the correlation between classes.\n",
    "\n",
    "- **predict_log_proba(X):** This method computes the logarithm of the probability estimates for each class for all samples in X. It returns an array where each row corresponds to a sample, and each column corresponds to a class, containing the logarithm of the probability of each class for each sample.\n",
    "\n",
    "- **predict_proba(X):** This method computes the probability estimates for each class for all samples in X. It returns an array where each row corresponds to a sample, and each column corresponds to a class, containing the probability of each class for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca73c97",
   "metadata": {},
   "source": [
    "**predict_joint_log_proba(X_test):**\n",
    "\n",
    "This method computes the logarithm of the joint probability density function for the samples in `X_test`.\n",
    "The joint probability density function represents the probability that a sample belongs to each class, taking into account the correlation between classes.\n",
    "`joint_log_proba` will be a 2D array where each row corresponds to a sample and each column corresponds to a class, containing the logarithm of the joint probability of each class for each sample.\n",
    "\n",
    "**predict_log_proba(X_test):**\n",
    "\n",
    "This method computes the logarithm of the probability estimates for each class for all samples in `X_test`.\n",
    "`log_proba` will be a 2D array where each row corresponds to a sample and each column corresponds to a class, containing the logarithm of the probability of each class for each sample.\n",
    "\n",
    "**predict_proba(X_test):**\n",
    "\n",
    "This method computes the probability estimates for each class for all samples in `X_test`.\n",
    "`proba` will be a 2D array where each row corresponds to a sample and each column corresponds to a class, containing the probability of each class for each sample.\n",
    "\n",
    "These methods are useful for understanding the model's confidence in its predictions and for further analysis of the model's behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f288f19",
   "metadata": {},
   "source": [
    "### Explanation of Using Logarithms for Probabilities in Machine Learning\n",
    "\n",
    "Using logarithms when dealing with probabilities, especially in machine learning models like Naive Bayes, offers several advantages:\n",
    "\n",
    "1. **Numerical Stability**: Multiplying small probabilities together can lead to numerical underflow issues. Taking the logarithm of probabilities helps avoid this problem by transforming multiplication into addition, which is more numerically stable.\n",
    "\n",
    "2. **Computational Efficiency**: Computing the logarithm of probabilities can be computationally more efficient, especially with large datasets or models with many parameters.\n",
    "\n",
    "3. **Interpretability**: Logarithms make it easier to interpret the relative magnitudes of probabilities. Small probabilities have large negative logarithms, while probabilities close to one have logarithms close to zero, facilitating easier comparison of likelihoods.\n",
    "\n",
    "4. **Prediction**: Logarithmizing probabilities doesn't alter the model's predictive capabilities. The relative ordering of probabilities remains the same, allowing for stable and efficient prediction. \n",
    "\n",
    "In summary, using logarithms of probabilities instead of directly computing probabilities enhances numerical stability, computational efficiency, and interpretability in machine learning models without affecting their predictive performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824b6e1-8179-489b-8e27-1b8367bf1ac6",
   "metadata": {},
   "source": [
    "**Q3bis** : Have a look at some basic implementation of the Naive Bayes algorithm \n",
    "\n",
    "https://www.python-engineer.com/courses/mlfromscratch/05_naivebayes/\n",
    "\n",
    "(a) Complete the code in order to compute the posterior membership probabilities (and not only the class with the highest posterior probability)\n",
    "\n",
    "(b) Also consider directly compute the log of the pdf rather than the pdf for numerical stability\n",
    "\n",
    "(c) What modification should you consider to consider categorical features?\n",
    "\n",
    "(d) Give some ideas of how it would be possible to consider mixed type of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2608aad0-a056-4e6a-a29b-a6f0dfe380df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class NaiveBayes:\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "#         self._classes = np.unique(y)\n",
    "#         n_classes = len(self._classes)\n",
    "\n",
    "#         # calculate mean, var, and prior for each class\n",
    "#         self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "#         self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "#         self._priors =  np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "#         for idx, c in enumerate(self._classes):\n",
    "#             X_c = X[y==c]\n",
    "#             self._mean[idx, :] = X_c.mean(axis=0)\n",
    "#             self._var[idx, :] = X_c.var(axis=0)\n",
    "#             self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         y_pred = [self._predict(x) for x in X]\n",
    "#         return np.array(y_pred)\n",
    "\n",
    "#     def _predict(self, x):\n",
    "#         posteriors = []\n",
    "\n",
    "#         # calculate posterior probability for each class\n",
    "#         for idx, c in enumerate(self._classes):\n",
    "#             prior = np.log(self._priors[idx])\n",
    "#             posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "#             posterior = prior + posterior\n",
    "#             posteriors.append(posterior)\n",
    "\n",
    "#         # return class with highest posterior probability\n",
    "#         return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "\n",
    "#     def _pdf(self, class_idx, x):\n",
    "#         mean = self._mean[class_idx]\n",
    "#         var = self._var[class_idx]\n",
    "#         numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
    "#         denominator = np.sqrt(2 * np.pi * var)\n",
    "#         return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb96ee7-7ef5-4e24-93b2-c9e6226354f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c1f6c-0388-43b8-956b-6450c8052dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bafaf5b3-01b8-4370-895e-af6141e3d48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (b) Also consider directly compute the log of the pdf rather than the pdf for numerical stability\n",
    "\n",
    "# (c) What modification should you consider to consider categorical features?\n",
    "\n",
    "# (d) Give some ideas of how it would be possible to consider mixed type of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020689f",
   "metadata": {},
   "source": [
    "-To consider categorical features in the Naive Bayes classifier, you need to modify the **_pdf** method to handle categorical features appropriately. Currently, the _pdf method assumes Gaussian distributions for continuous features. **For categorical features, you can calculate probabilities directly based on the counts of each category within each class.**\n",
    "\n",
    "-\n",
    "- This modification checks if any of the features in x are categorical. If there are categorical features, it calculates the probability of each category within the class directly based on counts. Otherwise, it uses the Gaussian PDF calculation for continuous features.\n",
    "\n",
    "- \n",
    "\n",
    "- For mixed types of data (i.e., a combination of categorical and continuous features), you would need to preprocess the data accordingly. One common approach is to use one-hot encoding for categorical features and leave continuous features as they are. After preprocessing, you can use the modified Naive Bayes classifier to handle both types of features appropriately. Additionally, you might consider using different types of Naive Bayes variants (e.g., GaussianNB for continuous features, CategoricalNB for categorical features) and combining their predictions through techniques like stacking or voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18311926-b910-4faa-8570-c169ada6612e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8612594-df92-46f3-9dab-0c32bcd2d9fb",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7066a-8cfc-49c0-860a-e3ffeb408543",
   "metadata": {},
   "source": [
    "**Q4**: Use `KBinsDiscretizer` of `sklearn.preprocessing` in order to discrtize the iris data with `n_bins=2`, then train a naive bayes classifier in this discretized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca97dc9a-0596-4fa0-b4b9-646aa345103a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0d493e8-c269-43c3-ac85-2866f4636af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  to discritise the data we are using KBinsDiscretizer \n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform', subsample=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43975a05",
   "metadata": {},
   "source": [
    "### KBinsDiscretizer for Discretizing Continuous Features\n",
    "\n",
    "KBinsDiscretizer from sklearn.preprocessing is used to discretize continuous features into discrete bins. It divides each feature into equally spaced bins (or intervals) and represents the values within each bin with a single value (e.g., the mean, median, or bin edge).\n",
    "\n",
    "#### Key Parameters:\n",
    "\n",
    "- **n_bins**: The number of bins to discretize each feature into.\n",
    "- **encode**: Specifies how to encode the discretized bins. Possible values are 'onehot', 'onehot-dense', and 'ordinal'.\n",
    "- **strategy**: The strategy used to define the widths of the bins. Possible values are 'uniform', 'quantile', and 'kmeans'.\n",
    "\n",
    "#### Bin Definition Strategies:\n",
    "\n",
    "1. **Equal Width Binning ('uniform')**: Divides the range of each feature into n_bins equally spaced intervals.\n",
    "\n",
    "2. **Equal Frequency Binning ('quantile')**: Divides the range of each feature into n_bins intervals, each containing approximately the same number of samples.\n",
    "\n",
    "3. **K-Means Binning ('kmeans')**: Uses the k-means algorithm to find n_bins cluster centers for each feature and then uses these centers to define the intervals.\n",
    "\n",
    "#### Encoding:\n",
    "\n",
    "After discretizing the features, KBinsDiscretizer can encode the bins in different ways:\n",
    "\n",
    "- **'onehot'**: Encode the bins using one-hot encoding.\n",
    "- **'onehot-dense'**: Encode the bins using one-hot encoding and return a dense array.\n",
    "- **'ordinal'**: Encode the bins as integers (0 to n_bins - 1).\n",
    "\n",
    "Overall, KBinsDiscretizer is useful for preprocessing continuous features before feeding them into machine learning models that may perform better with discrete features or when you want to simplify the representation of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc575ae",
   "metadata": {},
   "source": [
    "- it discretizes the features into 3 bins per feature using KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb503b9d-2c6b-481d-8676-9e38699eb9c1",
   "metadata": {},
   "source": [
    "**Q5**: Idem but with `n_bins = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed2786fc-4089-476d-ad5b-68b73dab8910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train1=est.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef30a013-f35b-4bab-aeb8-3ed894d170fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test1 = est.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2942ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88ee8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a631b9f8-1e61-4481-ad7e-bd18ec41861f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "classifier1 = CategoricalNB()\n",
    "classifier1.fit(X_train1, y_train)\n",
    "y_pred = classifier1.predict(X_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0aea825c-3cb3-4f4d-b14c-4184afb207e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "accuracy = classifier1.score(X_test1, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f65f9e03-fdc3-48b0-b827-487b834f0dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09844007, -2.77258872, -3.4657359 ],\n",
       "        [-1.5260563 , -0.30228087, -3.13549422],\n",
       "        [-3.36729583, -0.65924563, -0.80234647]]),\n",
       " array([[-2.77258872, -0.52129692, -1.06784063],\n",
       "        [-0.57054486, -0.93826964, -3.13549422],\n",
       "        [-1.42138568, -0.47692407, -1.98100147]]),\n",
       " array([[-0.06453852, -3.4657359 , -3.4657359 ],\n",
       "        [-2.44234704, -0.19105524, -2.44234704],\n",
       "        [-3.36729583, -2.67414865, -0.10919929]]),\n",
       " array([[-0.06453852, -3.4657359 , -3.4657359 ],\n",
       "        [-3.13549422, -0.13976194, -2.44234704],\n",
       "        [-3.36729583, -2.67414865, -0.10919929]])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1.feature_log_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699efc5",
   "metadata": {},
   "source": [
    "### Interpretation of feature_log_prob_ Attribute in Naive Bayes Classifiers\n",
    "\n",
    "The `feature_log_prob_` attribute in scikit-learn's Naive Bayes classifiers contains the logarithm of the probabilities of features given a class. It represents the log likelihood of each feature given each class, which is computed during the training of the classifier.\n",
    "\n",
    "#### Interpretation for Multinomial Naive Bayes Classifier:\n",
    "\n",
    "For a Multinomial Naive Bayes classifier, `feature_log_prob_` contains the logarithm of the probabilities of each feature occurring in each class. Each row corresponds to a class, and each column corresponds to a feature. So, `feature_log_prob_[i, j]` represents the logarithm of the probability of feature j occurring in class i.\n",
    "\n",
    "#### Interpretation for Gaussian Naive Bayes Classifier:\n",
    "\n",
    "For a Gaussian Naive Bayes classifier, `feature_log_prob_` contains the logarithm of the probability density function (PDF) of each feature given each class. Again, each row corresponds to a class, and each column corresponds to a feature. So, `feature_log_prob_[i, j]` represents the logarithm of the PDF of feature j in class i.\n",
    "\n",
    "Overall, `feature_log_prob_` provides valuable information about the likelihood of each feature given each class, which is essential for making predictions in Naive Bayes classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f103385-22fc-4b69-ac8f-ee77fa9ffe6e",
   "metadata": {},
   "source": [
    "**Q6**: Compare the performances of the different approches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2ca73-951e-4202-8f84-6df608cc0361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffec52-89c5-4bd0-948f-baf6fa272569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb91843c-4d08-4b8f-82ee-11aa584e37c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier2 = BernoulliNB()\n",
    "classifier2.fit(X_train1, y_train)\n",
    "y_pred = classifier1.predict(X_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "282aa582-ec9a-449b-ab96-b96e9ddd2df4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6., 28.,  0.,  0.],\n",
       "       [18., 13., 20., 20.],\n",
       "       [26., 25., 26., 26.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2.feature_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ce48a",
   "metadata": {},
   "source": [
    "- The **feature_count_** attribute in scikit-learn's Naive Bayes classifiers represents the count of each feature in each class. It is computed during the training of the classifier and is used to calculate the probabilities of features given a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e9dde76-3ed8-4b34-b471-0d22b2c0cf11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = classifier2.score(X_test1, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57b3e2-d5ee-4999-9153-a719af33a59c",
   "metadata": {},
   "source": [
    "## Application on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc362d6-b032-48ca-9d81-4e9532f6d666",
   "metadata": {},
   "source": [
    "**Q7**: Apply the Naive Bayes classifier on the MNIST dataset and comment its performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5e1a0dad-a9b3-4b6e-9a9e-54e29516d00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "46806e1d-6cfd-4f40-9e20-e73c172aa8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 1797\\n:Number of Attributes: 64\\n:Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n:Missing Attribute Values: None\\n:Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n:Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n  Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n  Graduate Studies in Science and Engineering, Bogazici University.\\n- E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n- Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n  Linear dimensionalityreduction using relevance weighted LDA. School of\\n  Electrical and Electronic Engineering Nanyang Technological University.\\n  2005.\\n- Claudio Gentile. A New Approximate Maximal Margin Classification\\n  Algorithm. NIPS. 2000.\\n\\n|details-end|\\n\"}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4517cab-2b5d-413b-9e92-a2854d010e9b",
   "metadata": {},
   "source": [
    "## 2. Linear and quadratic discriminant analysis\n",
    "\n",
    "The model now considered is not the naives Bayes, since it does not any more the assumption of conditional independence. It is however still a generative classifier since it models the distribution of $(x,y)$ then it deduce the distribution of $y|x$ using the Bayes theorem. \n",
    "\n",
    "More information can be found on : \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html\n",
    "\n",
    "The **linear discriminant analysis** assumes multivariate Gaussian distribution in each class, and moreover the covariance matrices are assumed to be the same in each class: \n",
    "$$\n",
    "x | y = k \\sim \\mathcal{N}(\\mu_k , \\Sigma)\n",
    "$$\n",
    "This results in a linear decision boundary. \n",
    "\n",
    "\n",
    "The **quadratic discriminant analysis** assumes multivariate Gaussian distribution in each class, and moreover the covariance matrices are assumed to be the same in each class: \n",
    "$$\n",
    "x | y = k \\sim \\mathcal{N}(\\mu_k , \\Sigma_k).\n",
    "$$\n",
    "This results in a quadratic decision boundary.\n",
    "\n",
    "**Remarks** : \n",
    "\n",
    "- Linear discriminant analysis can also be used for dimension reduction by looking for the most discriminant components (K-1 discriminant components if $K$ classes are considered) \n",
    "- In high dimensional setting the `shrinkage` parameter can be used in order to regularize the estimator of the covariance matrix, for example of the kind $(1-\\alpha)\\hat\\Sigma + \\alpha I$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df583830-fc15-4a9e-baf7-e96389fadf49",
   "metadata": {},
   "source": [
    "**Q8** : Fit a linear discriminant analysis on the MNIST dataset, try to optimize the value of the shrinkage parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e2f88-21c4-49d5-9e61-160d6f8b8500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc2a7a-8778-43de-aeaf-4651d18f6e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
